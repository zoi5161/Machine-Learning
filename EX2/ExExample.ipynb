{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tập 2\n",
    "\n",
    "\n",
    "Trần Gia Bảo - 22127034\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037582000000000004"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_coins = 1000\n",
    "num_flips = 10\n",
    "num_experiments = 100000\n",
    "\n",
    "def run_experiment():\n",
    "    flips = np.random.binomial(1, 0.5, (num_coins, num_flips))\n",
    "    fractions = flips.mean(axis=1)\n",
    "    v1 = fractions[0]\n",
    "    vrand = np.random.choice(fractions)\n",
    "    vmin = np.min(fractions)\n",
    "    return v1, vrand, vmin\n",
    "\n",
    "v1_values = []\n",
    "vrand_values = []\n",
    "vmin_values = []\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    v1, vrand, vmin = run_experiment()\n",
    "    v1_values.append(v1)\n",
    "    vrand_values.append(vrand)\n",
    "    vmin_values.append(vmin)\n",
    "\n",
    "avg_vmin = np.mean(vmin_values)\n",
    "avg_vmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task Division:\n",
    "1. Simulating 1,000 Coin Flips We use simulation to f lip 1,000 virtual coins 10 times each. Since the coins are fair, we will get half the tosses to land heads and half to land tails.\n",
    "2. Identifying Important Coins:\n",
    "- $c_1$: the first coin\n",
    "- $c_{rand}$: any random coin out of the 1,000\n",
    "- $c_{min}$: The coin with fewest heads, breaking ties by taking the first such coin encountered.\n",
    "3. Run the Experiment 100,000 Times One. It is sufficient to derive the statistical significance of our result by doing that many runs.\n",
    "4. Computation of $\\nu_1$, $\\nu_{rand}$ and $\\nu_{min}$ For each individual experiment we will calculate the proportion of Heads for all three of these coins. And since we repeat this experiment 100,000 times, we will also have an average of each:.\n",
    "\n",
    "Expected Result:\n",
    "\n",
    "Of most interest is $\\nu_{min}$ - the proportion heads for that coin which had fewest heads.\n",
    "\n",
    "Because we are only flipping each coin a paltry 10 times, it's quite possible-in fact expected-that at least one of the coins will get very few heads, even if all coins are fair. As a matter of fact, with that many coins being flipped, we're bound to see some extreme outcomes, hence it is likely that  will be close to zero.\n",
    "\n",
    "The result of this simulation is an average of about 0.038, because running only 10 flips, some of the coins will just happen to get, by pure random chance, say 1 or even 0 heads, so its fraction of heads ends up really low.\n",
    "\n",
    "0.038 is closer to 0.01; therefore, [b] 0.01 is the correct answer. The following was simulated by running the coin flips 100,000 times, and it was observed that the coin with a minimum amount of heads on average had a very low fraction of heads about 0.038 closest to option [b] 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "The Structure of the Assignment: \n",
    "1. Simulation of 1,000 Coin Tosses: A sample of 1,000 optical coins is taken and each coin is twisted 10 times. All these coins are optical coins and thus, any common coin knew that the probability of them all showing heads or all tails during any common flip was always 50%:\n",
    "2. Coins classification:\n",
    "- $c_1$: the first coin that is flipping;\n",
    "- $c_{rand}$: any of the 1000 coins after tossing all coins.\n",
    "- $c_{min}$ – from all the coins that have a minimum number of Heads. In case of the number of Heads being the same, take the first one.\n",
    "3. Conducting 100,000 Repetitions of the Experiment: The procedure will be done 100,000 times more to obtain $ \\nu_1 $ (the heads’ proportion for  $c_1$),  $\\nu_{rand}$  (the heads’ proportion for $c_{rand}$), and $ \\nu_{min} $ (the heads’ proportion for $ c_{min}$).\n",
    "4. Concentration inequalities for random variables such as Hoeffding’s inequality allow one to stratify the chances obstaining the obtained mean of $ \\nu_1 $ or $ \\nu_{rand} $ outside certain bounds. These values are 0.5 for a fair coin and expected value of the sample. It means that,  if the number of coin flips goes to infinity, then regardless of one’s desire, the fraction of heads will almost certainly assume any value as close to 0.5 as one wants.\n",
    "\n",
    "Simulation code of the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5007600000000001, 0.499678, 0.03800099999999999)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_coins = 1000\n",
    "num_flips = 10\n",
    "num_experiments = 100000\n",
    "\n",
    "def run_hoeffding_experiment():\n",
    "    flips = np.random.binomial(1, 0.5, (num_coins, num_flips))\n",
    "    fractions_of_heads = flips.mean(axis=1)\n",
    "    v1 = fractions_of_heads[0]\n",
    "    vrand = np.random.choice(fractions_of_heads)\n",
    "    vmin = np.min(fractions_of_heads)\n",
    "    return v1, vrand, vmin\n",
    "\n",
    "v1_values = []\n",
    "vrand_values = []\n",
    "vmin_values = []\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    v1, vrand, vmin = run_hoeffding_experiment()\n",
    "    v1_values.append(v1)\n",
    "    vrand_values.append(vrand)\n",
    "    vmin_values.append(vmin)\n",
    "\n",
    "avg_v1 = np.mean(v1_values)\n",
    "avg_vrand = np.mean(vrand_values)\n",
    "avg_vmin = np.mean(vmin_values)\n",
    "avg_v1, avg_vrand, avg_vmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of the simulation:\n",
    "- The mean value of $ \\nu_1 $ is 0.4996 - indeed very close to 0.5, as we should expect for a fair coin.\n",
    "- The mean value of $ \\nu_{rand} $ is 0.4999 - again close to 0.5 in correspondence with the behavior of a fair coin.\n",
    "- The average value of $ \\nu_{min} $ [fraction of heads for the coin with fewest heads] is 0.0377, much lower than 0.5, because we are choosing the coin that was worst performer in terms of heads.\n",
    "\n",
    "Conclusion:\n",
    "The Hoeffding Inequality does apply to $ c_1 $ and $ c_{rand} $ since each was chosen independently of the outcomes and its fraction of heads is close to 0.5. However, $ c_{min} $ was chosen for its extreme outcome-it had the fewest heads-so it is far from 0.5 and the inequality doesn't apply to $ c_{min}$\n",
    "\n",
    "Answer [d]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "Let's solve the problem step by step:\n",
    "\n",
    "We are given that there is a hypothesis h erring with probability $ \\mu $ while trying to fit a noisy version of a deterministic target function $f$. The Noisy version of f can be considered as above with the below probability distribution– refer the image, in which: With probability $\\lambda$, $ y = f(x) $ and $ 1 - \\lambda$, $ y \\neq f(x) $.\n",
    "\n",
    "At this point, we try to find the probability of error of h, whereby this error is considered in the occurrence of this noisy y.\n",
    "\n",
    "Method:\n",
    "\n",
    "1. Whenever, $ y = f(x) $. This happens with probability $ \\lambda $. Here, the error probability of h is $\\mu$ because it tries to fit g with the error probability of $ \\mu $\n",
    "2. When it is the case that $ y \\neq f(x) $\n",
    "This occurs with the probability of $ 1 - \\lambda$. Cases this case is labeled one considering that the probability the hypothesis h makes a mistake is $1 - \\mu$, because the h if wrong about the value of $f(x)$ will approximate y correctly as two wrongs make a right (two wrongs make a right).\n",
    "\n",
    "Now combining these two cases since they are mutually exclusive events, using the law of total probability.\n",
    "\n",
    "Error when $y = f(x)$ occurs with prob. $ \\lambda \\times \\mu$.\n",
    "\n",
    "Error when $ y \\neq f(x) $ has probability $ (1 - \\lambda) \\times (1 - \\mu)$.\n",
    "\n",
    "Hence, general expression for the probability of error will be:\n",
    "\n",
    "$ P(\\text{error}) = \\lambda \\cdot \\mu + (1 - \\lambda) \\cdot (1 - \\mu) $\n",
    "\n",
    "It carries the same formula as given in option [e] $(1−λ)∗(1−μ)+λ∗μ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "Let's break this problem down piece by piece.\n",
    "\n",
    "Recall the formula of the error probability we found above: \n",
    "\n",
    "$$P(\\text{error}) = \\lambda \\cdot \\mu + (1 - \\lambda) \\cdot (1 - \\mu)$$\n",
    "\n",
    "This time, we are to find the unique value of $ \\lambda $ for which the performance of $ h $ does not depend on $ \\mu $, that is the value of $ P(\\text{error}) $ does not change with $ \\mu $.\n",
    "\n",
    "Step by step solution:\n",
    "\n",
    "1. Then the probability of an error may be computed as : \n",
    "\n",
    "$$ P(\\text{error}) = \\lambda \\mu + (1 - \\lambda)(1 - \\mu) $$\n",
    "\n",
    "- We can expand this as:\n",
    "\n",
    "$$ P(\\text{error}) = \\lambda \\mu + (1 - \\lambda - \\mu + \\lambda \\mu) $$\n",
    "\n",
    "- This simplifies to: \n",
    "\n",
    "$$ P(\\text{error}) = \\lambda \\mu + 1 - \\lambda - \\mu + \\lambda \\mu $$\n",
    "\n",
    "$$ P(\\text{error}) = 1 - \\lambda - \\mu + 2 \\lambda \\mu $$\n",
    "\n",
    "2. Find the condition under which $\\mu $ becomes negligible:\n",
    "\n",
    "- It is obvious now, that for this expression to be zero the coefficient in front of $ \\mu $ has to be zero. Let us collect the terms which have been proportional to $ \\mu $: \n",
    "\n",
    "$$ P(\\text{error}) = 1 - \\lambda + \\mu(2 \\lambda - 1) $$\n",
    "\n",
    "- Henceforth, assume there is some other way to express $ P{error} $ in terms not involving the word $ \\mu $. Then we shall only need to solve the following equation:\n",
    "\n",
    "$$ 2 \\lambda - 1 = 0 $$\n",
    "\n",
    "$$ \\lambda = \\frac{1}{2}$$\n",
    "\n",
    "Conclusion: Again, the performance of $h$ does not depend on $\\mu$ when $\\lambda = 0.5$ and this in turn again is identical as option [b] 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\n",
    "This can be solved by following in some orderly steps and by emulating the process of classification using Linear Regression thereby calculating the in-sample error, $ E_{in} $. Following are the steps for this:.\n",
    "\n",
    "Step-by-Step Breakdown:\n",
    "1. Setting up the problem:\n",
    "- We choose d = 2; that is the points lie in two dimensions and the input space x is the square $[-1, 1] \\times [-1, 1]$.\n",
    "Here it is the target function g defined by the random line separating the space into two portions where one side of the line corresponds to +1 while the other part corresponds to -1.\n",
    "2. Define a target function:\n",
    "We randomly choose two points in the square $[-1, 1] \\times [-1, 1]$. The line through these two random choices determines our target function, $f$.\n",
    "- It will map any point to +1 or  -1 depending on which side of the line the update falls.\n",
    "3. Creating the Dataset\n",
    "Exercises In the following exercises we will generate N=100 random points $ x_n $ in the space $ [-1,1]\\times [-1,1]$.\n",
    "- The target function $g$ assigns a label $ y_n = \\pm 1 $ to each point $ x_n $.\n",
    "4. Using Linear Regression find the following:\n",
    "Linear Regression: We will be doing the best-fitting hypothesis $g$ to the given dataset D.\n",
    "- It involves solving for weights $ \\mathbf{w} $ which minimizes the squared error between the predicted outputs, $ g(x_n) $ and the actual output, $ y_n $:.\n",
    "5. Calculate the in-sample error:\n",
    "- Once we have learned g, we compute the in-sample error $ E_{in}$, the fraction of points in  D  that are misclassified by g. That is,\n",
    "\n",
    "$$ E_{in} = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbb{1} \\left( g(x_n) \\neq y_n \\right) $$\n",
    "\n",
    "where $\\mathbb{1}$ is the indictor function that counts 1 when the classification is wrong and 0 otherwise.\n",
    "\n",
    "6. Repeat the experiment:\n",
    "\n",
    "We repeat the whole procedure 1000 times and take an average of all the experiments as the value of $ E_{in} $.\n",
    "\n",
    "Implementation: Now by using these, let's write a code in Python to find the average $ E_{in}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03882000000000002"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_target_function():\n",
    "    p1, p2 = np.random.uniform(-1, 1, (2, 2))\n",
    "    A = p2[1] - p1[1]\n",
    "    B = p1[0] - p2[0]\n",
    "    C = p2[0] * p1[1] - p1[0] * p2[1]\n",
    "    return lambda x: np.sign(A * x[:, 0] + B * x[:, 1] + C)\n",
    "\n",
    "def linear_regression(X, Y):\n",
    "    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    w = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ Y\n",
    "    return w\n",
    "\n",
    "def hypothesis(X, w):\n",
    "    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    return np.sign(X_with_bias @ w)\n",
    "\n",
    "N = 100\n",
    "num_experiments = 1000\n",
    "E_in_total = 0\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    target_function = generate_target_function()\n",
    "    X = np.random.uniform(-1, 1, (N, 2))\n",
    "    Y = target_function(X)\n",
    "    w = linear_regression(X, Y)\n",
    "    \n",
    "    Y_pred = hypothesis(X, w)\n",
    "    E_in = np.mean(Y_pred != Y)\n",
    "    E_in_total += E_in\n",
    "\n",
    "average_E_in = E_in_total / num_experiments\n",
    "average_E_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the Code:\n",
    "\n",
    "- We create a target function in the shape of a line through two randomly created points.\n",
    "- The following experiments will generate a dataset of 100 points with their labels.\n",
    "- We employ Linear Regression in order to calculate the weights $ \\mathbf{w} $ for the hypothesis g.\n",
    "- We compute the in-sample error $ E_{in} $, comparing the estimated labels $ g(x_n) $ to the target labels $ y_n $.\n",
    "- We do this all 1000 times and compute the average $ E_{in} $. \n",
    "\n",
    "Expected Result:\n",
    "\n",
    "After running the code we should see that the average $ E_{in} $ is closest to [c] 0.01. Thus indicating on average Linear Regression misclassifies about 1% of the in-sample points.\n",
    "\n",
    "Conclusion: The value closest, to the mean $ E_{in} $ is [c] 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "Let's start from the result established in Problem 5 in the Linear Regression setting. Using the latter let's compute an estimate of the out of sample error, $ E_{out} $ of the hypothesis $g$ that we obtained in problem 5.\n",
    "\n",
    "What we must perform is:\n",
    "1. Generating 1000 new points: Using the obtained hypothesis $g$ with Linear Regression, we will generate 1000 new random points - out-of-sample points - in the same input space $[-1, 1] \\times [-1, 1]$.\n",
    "\n",
    "2. Classify $g$ for the new data points: Using the hypothesis, $g$ developed in the previous problem, determine the labels of these new points.\n",
    "\n",
    "3. Calculate $ E_{out} $: Out of sample error $ E_{out} $ is the fraction of misclassified points out of 1000 out-of-sample points :\n",
    "\n",
    "$$ E_{out} = \\frac{\\text{Number of misclassified points}}{\\text{Total number of out-of-sample points}} $$\n",
    "\n",
    "4. Repeat for 1000 runs: The usually followed method is running the experiment 1000 times and taking an average of the value of $ E_{out} $ from all experiments.\n",
    "\n",
    "Code Implementation: We will continue the previous code by adding this new section that will calculate $ E_{out} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04808500000000002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_out_sample_points = 1000\n",
    "E_out_total = 0\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    target_function = generate_target_function()\n",
    "    X_in = np.random.uniform(-1, 1, (N, 2))\n",
    "    Y_in = target_function(X_in)\n",
    "    \n",
    "    w = linear_regression(X_in, Y_in)\n",
    "    \n",
    "    X_out = np.random.uniform(-1, 1, (num_out_sample_points, 2))\n",
    "    Y_out_true = target_function(X_out)\n",
    "    \n",
    "    Y_out_pred = hypothesis(X_out, w)\n",
    "    \n",
    "    E_out = np.mean(Y_out_pred != Y_out_true)\n",
    "    E_out_total += E_out\n",
    "\n",
    "average_E_out = E_out_total / num_experiments\n",
    "average_E_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the code:\n",
    "\n",
    "- Generate in each experiment 1000 new out-of-sample points and compute for them their true labels using the target function  $f$. \n",
    "- We'll use the following hypothesis $g$ we got from Linear Regression to make predictions on those out-of-sample points.\n",
    "- We can then estimate the out-of-sample error, comparing the predicted labels against the true labels. It does this by repeating the above process 1000 times where at each turn we estimate the average $ E_{out} $.\n",
    "\n",
    "Expected Result:\n",
    "\n",
    "Thus, after running this code we will find that the average $ E_{out} $ is closest to [c] 0.01. This perhaps could be understood to mean that average out of sample error being about 1% agrees with in-sample error obtained earlier.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "The value closest to the average $ E_{out} $ is [c] 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7:\n",
    "This is best approached by fully explaining each step of how one would do PLA with Linear Regression initialization for the solution.\n",
    "\n",
    "Problem Overview:\n",
    "\n",
    "The target function f is generated by randomly drawing a line that passes through two randomly chosen points in the area $[-1, 1]\\times[-1, 1]$. This line defines how the plane is divided into two halves and assigns each half to one class, +1 or -1.\n",
    "2. Create Dataset : D\n",
    "- Set N = 10 so that the ground truth would consist of 10 random points in $[-1, 1]\\times[-1, 1]$. \n",
    "- Label these points using classification labels based on which side of the line f.\n",
    "3.\tTrain Linear Regression:\n",
    "- Use Linear Regression to find a weight vector $w_{\\text{LR}}$ which approximates the separation of the dataset D.\n",
    "4. Use $w_{\\text{LR}}$ as Starting Weights for PLA:\n",
    "- PLA is initialized to the Perceptron Learning Algorithm with. \n",
    "- PLA iteratively update weights: to get weights which classify all points in, where in every step take any point that is misclassified, choose it at random and update the weights.\n",
    "5. Calculate Average Number of Iterations:\n",
    "- Repeat the above steps 1000 times, averaging the results for the number of PLA iterations to converge.\n",
    "\n",
    "Because there are choices, it seems we should take an average number of iterations and see which one of the answer choices is closest.\n",
    "\n",
    "Explanation of Expected Outcomes: The weights $w_{\\text{LR}}$ from Linear Regression are likely to already be rather close to the final separating weights. PLA should thus take much fewer iterations to converge. Therefore, the average number of iterations is expected to be rather low, could be around 1. However, as the problem wants such a setting to be run for 1000 runs, the simulation will return the exact empirical average of PLA iterations. Code Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.854"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_target_function():\n",
    "    points = np.random.uniform(-1, 1, (2, 2))\n",
    "    slope = (points[1, 1] - points[0, 1]) / (points[1, 0] - points[0, 0])\n",
    "    intercept = points[0, 1] - slope * points[0, 0]\n",
    "    return np.array([-intercept, slope, -1])\n",
    "\n",
    "def generate_dataset(N, target_weights):\n",
    "    X = np.hstack((np.ones((N, 1)), np.random.uniform(-1, 1, (N, 2))))\n",
    "    y = np.sign(X @ target_weights)\n",
    "    return X, y\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    pseudo_inverse = np.linalg.pinv(X)\n",
    "    return pseudo_inverse @ y\n",
    "\n",
    "def perceptron_learning_algorithm(X, y, initial_weights):\n",
    "    weights = initial_weights.copy()\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        predictions = np.sign(X @ weights)\n",
    "        misclassified_points = np.where(predictions != y)[0]\n",
    "        if len(misclassified_points) == 0:\n",
    "            break\n",
    "        random_index = np.random.choice(misclassified_points)\n",
    "        weights += y[random_index] * X[random_index]\n",
    "        iterations += 1\n",
    "    return iterations\n",
    "\n",
    "N = 10\n",
    "num_runs = 1000\n",
    "iterations_list = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    target_weights = generate_target_function()\n",
    "    X, y = generate_dataset(N, target_weights)\n",
    "    initial_weights = linear_regression(X, y)\n",
    "    iterations = perceptron_learning_algorithm(X, y, initial_weights)\n",
    "    iterations_list.append(iterations)\n",
    "\n",
    "average_iterations = np.mean(iterations_list)\n",
    "average_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this simulation will result in an average PLA iterations which will assist to prove whether [a] 1 is truly the closest answer from amongst the alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "To estimate the in-sample error of classification  $E_{in}$  for a linear regression model without any transformation:\n",
    "1. Generate data according to the given feature vector: (1,  $x_1$, $x_2$ ), and corresponding labels.\n",
    "2. Execute Linear Regression to calculate weights $w$.\n",
    "3. Classify by the sign of the regression output; compute the in-sample classification error $E_{in}$ in by comparing the sign of predictions to the true labels.\n",
    "4. Repeat this experiment 1000 times and take the average $E_{in}$ to reduce the variance.\n",
    "\n",
    "We will continue this calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43895"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_experiments = 1000\n",
    "num_samples = 100\n",
    "\n",
    "errors = []\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    X = np.random.uniform(-1, 1, (num_samples, 2))\n",
    "    X_with_bias = np.c_[np.ones(num_samples), X]\n",
    "    target_function = np.sign(X[:, 0] ** 2 + X[:, 1] ** 2 - 0.6)\n",
    "    \n",
    "    noise = np.random.rand(num_samples) < 0.1\n",
    "    target_function[noise] *= -1\n",
    "\n",
    "    pseudo_inverse = np.linalg.pinv(X_with_bias)\n",
    "    w = pseudo_inverse @ target_function\n",
    "\n",
    "    predictions = np.sign(X_with_bias @ w)\n",
    "    ein = np.mean(predictions != target_function)\n",
    "    \n",
    "    errors.append(ein)\n",
    "\n",
    "average_ein = np.mean(errors)\n",
    "average_ein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average over 1000 experiments of the in-sample error of classification is about 0.5. Thus the correct answer is: [d] 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "First, to solve the nonlinearly transformed linear regression problem, we have to transform each data point with the feature vector  $(1, x_1, x_2, x_1x_2, x_1^2, x_2^2)$. Then, we run the linear regression to get the weight vector.  That is, for any set of given five candidate hypotheses, one has to identify which one of these hypotheses yields the nearest approximation by returning a maximum probability of agreement on any randomly chosen point. Absolute agreements between the hypothesis and the target function are estimated for 1000 samples with the average agreement rate over 10 runs for stability.\n",
    "\n",
    "Code Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.8514999999999999,\n",
       " 'b': 0.6462000000000001,\n",
       " 'c': 0.6458,\n",
       " 'd': 0.5902000000000001,\n",
       " 'e': 0.5272}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_experiments = 10\n",
    "num_samples = 1000\n",
    "\n",
    "candidate_weights = {\n",
    "    'a': np.array([-1, -0.05, 0.08, 0.13, 1.5, 1.5]),\n",
    "    'b': np.array([-1, -0.05, 0.08, 0.13, 1.5, 15]),\n",
    "    'c': np.array([-1, -0.05, 0.08, 0.13, 15, 1.5]),\n",
    "    'd': np.array([-1, -1.5, 0.08, 0.13, 0.05, 0.05]),\n",
    "    'e': np.array([-1, -0.05, 0.08, 1.5, 0.15, 0.15])\n",
    "}\n",
    "\n",
    "agreement_counts = {key: 0 for key in candidate_weights.keys()}\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    X = np.random.uniform(-1, 1, (num_samples, 2))\n",
    "    X_transformed = np.c_[np.ones(num_samples), X, X[:, 0] * X[:, 1], X[:, 0] ** 2, X[:, 1] ** 2]\n",
    "    target_labels = np.sign(X[:, 0] ** 2 + X[:, 1] ** 2 - 0.6)\n",
    "    \n",
    "    noise = np.random.rand(num_samples) < 0.1\n",
    "    target_labels[noise] *= -1\n",
    "    pseudo_inverse = np.linalg.pinv(X_transformed)\n",
    "    w = pseudo_inverse @ target_labels\n",
    "    \n",
    "    for key, hypothesis_weights in candidate_weights.items():\n",
    "        hypothesis_predictions = np.sign(X_transformed @ hypothesis_weights)\n",
    "        agreement_rate = np.mean(hypothesis_predictions == target_labels)\n",
    "        agreement_counts[key] += agreement_rate\n",
    "\n",
    "average_agreements = {key: agreement_counts[key] / num_experiments for key in candidate_weights.keys()}\n",
    "average_agreements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the experiments were run, it became clear that the hypothesis that had the most average agreements, hence closest to our model, was the following:\n",
    "\n",
    "[a] $ g(x_1, x_2) = \\text{sign}(-1 - 0.05x_1 + 0.08x_2 + 0.13x_1x_2 + 1.5x_1^2 + 1.5x_2^2) $\n",
    "\n",
    "This hypothesis best fits the weights returned in our solution of the regression, by far more than any of the other options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\n",
    "To estimate the classification out-of-sample error $ E_{\\text{out}} $ for our chosen hypothesis from the prior analysis, we proceed by generating a fresh set of 1000 data points with the feature vector $ (1, x_1, x_2, x_1 x_2, x_1^2, x_2^2) $. As before, 10% of the labels are randomly flipped to introduce noise, mimicking real-world imperfections. Finally, we calculate $ E_{\\text{out}} $ by comparing the hypothesis predictions against the actual noisy labels, averaging this error over 1000 runs to minimize variation and achieve a stable estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.143701"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_out_experiments = 1000\n",
    "num_out_samples = 1000\n",
    "\n",
    "best_hypothesis_weights = np.array([-1, -0.05, 0.08, 0.13, 1.5, 1.5])\n",
    "\n",
    "out_sample_errors = []\n",
    "\n",
    "for _ in range(num_out_experiments):\n",
    "    X_out = np.random.uniform(-1, 1, (num_out_samples, 2))\n",
    "    X_out_transformed = np.c_[np.ones(num_out_samples), X_out, X_out[:, 0] * X_out[:, 1], X_out[:, 0] ** 2, X_out[:, 1] ** 2]\n",
    "    \n",
    "    target_labels_out = np.sign(X_out[:, 0] ** 2 + X_out[:, 1] ** 2 - 0.6)\n",
    "    noise_out = np.random.rand(num_out_samples) < 0.1\n",
    "    target_labels_out[noise_out] *= -1\n",
    "\n",
    "    predictions_out = np.sign(X_out_transformed @ best_hypothesis_weights)\n",
    "    e_out = np.mean(predictions_out != target_labels_out)\n",
    "    out_sample_errors.append(e_out)\n",
    "    \n",
    "average_eout = np.mean(out_sample_errors)\n",
    "average_eout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closest value to the average out-of-sample error $ E_{\\text{out}} $, based on 1000 runs, is approximately 0.1. Thus, the correct answer is: [b] 0.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
